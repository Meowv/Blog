---
title: Scrapyæ¡†æ¶
author: é˜¿æ˜Ÿğ‘·ğ’ğ’–ğ’”
date: 2019-07-16 19:51:00
categories: Python
tags:
  - Scrapy
  - çˆ¬è™«
---

## Scrapy æ¡†æ¶ä»‹ç»

å†™ä¸€ä¸ªçˆ¬è™«ï¼Œéœ€è¦åšå¾ˆå¤šçš„äº‹æƒ…ï¼Œæ¯”å¦‚ï¼šå‘é€ç½‘ç»œè¯·æ±‚ã€æ•°æ®è§£æã€æ•°æ®å­˜å‚¨ã€ååçˆ¬è™«æœºåˆ¶\(ip ä»£ç†ï¼Œè®¾ç½®è¯·æ±‚å¤´ç­‰\)ã€å¼‚æ­¥è¯·æ±‚ç­‰ç­‰ã€‚è¿™äº›å·¥ä½œå¦‚æœæ¯æ¬¡éƒ½è¦è‡ªå·±ä»é›¶å¼€å§‹å†™çš„è¯ï¼Œæ¯”è¾ƒæµªè´¹æ—¶é—´ã€‚å› æ­¤ scrapy æŠŠä¸€äº›åŸºç¡€çš„ä¸œè¥¿éƒ½å°è£…å¥½äº†ï¼Œåœ¨ scrapy æ¡†æ¶ä¸Šå¼€å‘çˆ¬è™«å¯ä»¥å˜å¾—æ›´åŠ çš„é«˜æ•ˆï¼Œçˆ¬å–æ•ˆç‡å’Œå¼€å‘æ•ˆç‡å¾—åˆ°æå‡ã€‚

## Scrapy æ¡†æ¶æ¨¡å—åŠŸèƒ½

- Scrapy Engineï¼ˆå¼•æ“ï¼‰ï¼šScrapy æ¡†æ¶çš„æ ¸å¿ƒéƒ¨åˆ†ã€‚è´Ÿè´£åœ¨ Spider å’Œ ItemPipelineã€Downloaderã€Scheduler ä¸­é—´é€šä¿¡ã€ä¼ é€’æ•°æ®ç­‰ã€‚
- Spiderï¼ˆçˆ¬è™«ï¼‰ï¼šå‘é€éœ€è¦çˆ¬å–çš„é“¾æ¥ç»™å¼•æ“ï¼Œæœ€åå¼•æ“æŠŠå…¶ä»–æ¨¡å—è¯·æ±‚å›æ¥çš„æ•°æ®å†å‘é€ç»™çˆ¬è™«ï¼Œçˆ¬è™«å°±å»è§£ææƒ³è¦çš„æ•°æ®ã€‚è¿™ä¸ªéƒ¨åˆ†æ˜¯æˆ‘ä»¬å¼€å‘è€…è‡ªå·±å†™çš„ï¼Œå› ä¸ºè¦çˆ¬å–å“ªäº›é“¾æ¥ï¼Œé¡µé¢ä¸­çš„å“ªäº›æ•°æ®æ˜¯æˆ‘ä»¬éœ€è¦çš„ï¼Œéƒ½æ˜¯ç”±ç¨‹åºå‘˜è‡ªå·±å†³å®šã€‚
- Schedulerï¼ˆè°ƒåº¦å™¨ï¼‰ï¼šè´Ÿè´£æ¥æ”¶å¼•æ“å‘é€è¿‡æ¥çš„è¯·æ±‚ï¼Œå¹¶æŒ‰ç…§ä¸€å®šçš„æ–¹å¼è¿›è¡Œæ’åˆ—å’Œæ•´ç†ï¼Œè´Ÿè´£è°ƒåº¦è¯·æ±‚çš„é¡ºåºç­‰ã€‚
- Downloaderï¼ˆä¸‹è½½å™¨ï¼‰ï¼šè´Ÿè´£æ¥æ”¶å¼•æ“ä¼ è¿‡æ¥çš„ä¸‹è½½è¯·æ±‚ï¼Œç„¶åå»ç½‘ç»œä¸Šä¸‹è½½å¯¹åº”çš„æ•°æ®å†äº¤è¿˜ç»™å¼•æ“ã€‚
- Item Pipelineï¼ˆç®¡é“ï¼‰ï¼šè´Ÿè´£å°† Spiderï¼ˆçˆ¬è™«ï¼‰ä¼ é€’è¿‡æ¥çš„æ•°æ®è¿›è¡Œä¿å­˜ã€‚å…·ä½“ä¿å­˜åœ¨å“ªé‡Œï¼Œåº”è¯¥çœ‹å¼€å‘è€…è‡ªå·±çš„éœ€æ±‚ã€‚
- Downloader Middlewaresï¼ˆä¸‹è½½ä¸­é—´ä»¶ï¼‰ï¼šå¯ä»¥æ‰©å±•ä¸‹è½½å™¨å’Œå¼•æ“ä¹‹é—´é€šä¿¡åŠŸèƒ½çš„ä¸­é—´ä»¶ã€‚
- Spider Middlewaresï¼ˆSpider ä¸­é—´ä»¶ï¼‰ï¼šå¯ä»¥æ‰©å±•å¼•æ“å’Œçˆ¬è™«ä¹‹é—´é€šä¿¡åŠŸèƒ½çš„ä¸­é—´ä»¶ã€‚

## Scrapy å®‰è£…å’Œæ–‡æ¡£

- å®‰è£…ï¼šé€šè¿‡ `pip install scrapy` å³å¯å®‰è£…ã€‚
  - åœ¨ ubuntu ä¸Šå®‰è£… scrapy ä¹‹å‰ï¼Œéœ€è¦å…ˆå®‰è£…ä»¥ä¸‹ä¾èµ–ï¼š`sudo apt-get install python3-dev build-essential python3-pip libxml2-dev libxslt1-dev zlib1g-dev libffi-dev libssl-dev`ï¼Œç„¶åå†é€šè¿‡ `pip install scrapy` å®‰è£…ã€‚
  - å¦‚æœåœ¨ windows ç³»ç»Ÿä¸‹ï¼Œæç¤ºè¿™ä¸ªé”™è¯¯ ModuleNotFoundError: No module named 'win32api'ï¼Œé‚£ä¹ˆä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å¯ä»¥è§£å†³ï¼š`pip install pypiwin32`ã€‚
- Scrapy å®˜æ–¹æ–‡æ¡£ï¼š[http://doc.scrapy.org/en/latest](http://doc.scrapy.org/en/latest)
- Scrapy ä¸­æ–‡æ–‡æ¡£ï¼š[http://scrapy-chs.readthedocs.io/zh_CN/latest/index.html](http://scrapy-chs.readthedocs.io/zh_CN/latest/index.html)

## Scrapy å¿«é€Ÿå…¥é—¨

### åˆ›å»ºé¡¹ç›®

è¦ä½¿ç”¨ Scrapy æ¡†æ¶åˆ›å»ºé¡¹ç›®ï¼Œéœ€è¦é€šè¿‡å‘½ä»¤æ¥åˆ›å»ºã€‚é¦–å…ˆè¿›å…¥åˆ°ä½ æƒ³æŠŠè¿™ä¸ªé¡¹ç›®å­˜æ”¾çš„ç›®å½•ã€‚ç„¶åä½¿ç”¨ä»¥ä¸‹å‘½ä»¤åˆ›å»ºï¼š

`scrapy startproject [é¡¹ç›®åç§°]`

### ç›®å½•ç»“æ„ä»‹ç»

- items.pyï¼šç”¨æ¥å­˜æ”¾çˆ¬è™«çˆ¬å–ä¸‹æ¥æ•°æ®çš„æ¨¡å‹ã€‚
- middlewares.pyï¼šç”¨æ¥å­˜æ”¾å„ç§ä¸­é—´ä»¶çš„æ–‡ä»¶ã€‚
- pipelines.pyï¼šç”¨æ¥å°† items çš„æ¨¡å‹å­˜å‚¨åˆ°æœ¬åœ°ç£ç›˜ä¸­ã€‚
- settings.pyï¼šæœ¬çˆ¬è™«çš„ä¸€äº›é…ç½®ä¿¡æ¯ï¼ˆæ¯”å¦‚è¯·æ±‚å¤´ã€å¤šä¹…å‘é€ä¸€æ¬¡è¯·æ±‚ã€ip ä»£ç†æ± ç­‰ï¼‰ã€‚
- scrapy.cfgï¼šé¡¹ç›®çš„é…ç½®æ–‡ä»¶ã€‚
- spiders åŒ…ï¼šä»¥åæ‰€æœ‰çš„çˆ¬è™«ï¼Œéƒ½æ˜¯å­˜æ”¾åˆ°è¿™ä¸ªé‡Œé¢ã€‚

### ä½¿ç”¨ Scrapy æ¡†æ¶çˆ¬å–ç³—äº‹ç™¾ç§‘æ®µå­ä¾‹å­

#### ä½¿ç”¨å‘½ä»¤åˆ›å»ºä¸€ä¸ªçˆ¬è™«

`scrapy gensipder qsbk "qiushibaike.com"`

åˆ›å»ºäº†ä¸€ä¸ªåå­—å«åš qsbk çš„çˆ¬è™«ï¼Œå¹¶ä¸”èƒ½çˆ¬å–çš„ç½‘é¡µåªä¼šé™åˆ¶åœ¨ qiushibaike.com è¿™ä¸ªåŸŸåä¸‹ã€‚

#### çˆ¬è™«ä»£ç è§£æ

```python
import scrapy

class QsbkSpider(scrapy.Spider):
    name = 'qsbk'
    allowed_domains = ['qiushibaike.com']
    start_urls = ['http://qiushibaike.com/']

    def parse(self, response):
        pass
```

å…¶å®è¿™äº›ä»£ç æˆ‘ä»¬å®Œå…¨å¯ä»¥è‡ªå·±æ‰‹åŠ¨å»å†™ï¼Œè€Œä¸ç”¨å‘½ä»¤ã€‚åªä¸è¿‡æ˜¯ä¸ç”¨å‘½ä»¤ï¼Œè‡ªå·±å†™è¿™äº›ä»£ç æ¯”è¾ƒéº»çƒ¦ã€‚

è¦åˆ›å»ºä¸€ä¸ª Spiderï¼Œé‚£ä¹ˆå¿…é¡»è‡ªå®šä¹‰ä¸€ä¸ªç±»ï¼Œç»§æ‰¿è‡ª scrapy.Spiderï¼Œç„¶ååœ¨è¿™ä¸ªç±»ä¸­å®šä¹‰ä¸‰ä¸ªå±æ€§å’Œä¸€ä¸ªæ–¹æ³•ã€‚

- nameï¼šè¿™ä¸ªçˆ¬è™«çš„åå­—ï¼Œåå­—å¿…é¡»æ˜¯å”¯ä¸€çš„ã€‚
- allow_domainsï¼šå…è®¸çš„åŸŸåã€‚çˆ¬è™«åªä¼šçˆ¬å–è¿™ä¸ªåŸŸåä¸‹çš„ç½‘é¡µï¼Œå…¶ä»–ä¸æ˜¯è¿™ä¸ªåŸŸåä¸‹çš„ç½‘é¡µä¼šè¢«è‡ªåŠ¨å¿½ç•¥ã€‚
- start_urlsï¼šçˆ¬è™«ä»è¿™ä¸ªå˜é‡ä¸­çš„ url å¼€å§‹ã€‚
- parseï¼šå¼•æ“ä¼šæŠŠä¸‹è½½å™¨ä¸‹è½½å›æ¥çš„æ•°æ®æ‰”ç»™çˆ¬è™«è§£æï¼Œçˆ¬è™«å†æŠŠæ•°æ®ä¼ ç»™è¿™ä¸ª parse æ–¹æ³•ã€‚è¿™ä¸ªæ˜¯ä¸ªå›ºå®šçš„å†™æ³•ã€‚è¿™ä¸ªæ–¹æ³•çš„ä½œç”¨æœ‰ä¸¤ä¸ªï¼Œç¬¬ä¸€ä¸ªæ˜¯æå–æƒ³è¦çš„æ•°æ®ã€‚ç¬¬äºŒä¸ªæ˜¯ç”Ÿæˆä¸‹ä¸€ä¸ªè¯·æ±‚çš„ urlã€‚

#### ä¿®æ”¹ settings.py ä»£ç 

åœ¨åšä¸€ä¸ªçˆ¬è™«ä¹‹å‰ï¼Œä¸€å®šè¦è®°å¾—ä¿®æ”¹ setttings.py ä¸­çš„è®¾ç½®ã€‚ä¸¤ä¸ªåœ°æ–¹æ˜¯å¼ºçƒˆå»ºè®®è®¾ç½®çš„ã€‚

- ROBOTSTXT_OBEY è®¾ç½®ä¸º Falseã€‚é»˜è®¤æ˜¯ Trueã€‚å³éµå®ˆæœºå™¨åè®®ï¼Œé‚£ä¹ˆåœ¨çˆ¬è™«çš„æ—¶å€™ï¼Œscrapy é¦–å…ˆå»æ‰¾ robots.txt æ–‡ä»¶ï¼Œå¦‚æœæ²¡æœ‰æ‰¾åˆ°ã€‚åˆ™ç›´æ¥åœæ­¢çˆ¬å–ã€‚
- DEFAULT_REQUEST_HEADERS æ·»åŠ  User-Agentã€‚è¿™ä¸ªä¹Ÿæ˜¯å‘Šè¯‰æœåŠ¡å™¨ï¼Œæˆ‘è¿™ä¸ªè¯·æ±‚æ˜¯ä¸€ä¸ªæ­£å¸¸çš„è¯·æ±‚ï¼Œä¸æ˜¯ä¸€ä¸ªçˆ¬è™«ã€‚

#### å®Œæˆçš„çˆ¬è™«ä»£ç 

##### çˆ¬è™«éƒ¨åˆ†ä»£ç 

```python
import scrapy
from scrapy.http.response.html import HtmlResponse
from scrapy.selector.unified import SelectorList
from qsbk.items import QsbkItem

class QsbkSpider(scrapy.Spider):
    name = 'qsbk_spider'
    allowed_domains = ['qiushibaike.com']
    start_urls = ['https://www.qiushibaike.com/text/page/1/']
    base_domain = 'https://www.qiushibaike.com'

    def parse(self, response):
        duanziDivs = contentLeft = response.xpath("//div[@id='content-left']/div")
        for duanzidiv in duanziDivs:
            author = duanzidiv.xpath(".//h2/text()").get().strip()
            content = duanzidiv.xpath(".//div[@class='content']//text()").getall()
            content = "".join(content).strip()

            # duanzi = {"author":author,"content":content}
            # yield duanzi

            item = QsbkItem(author=author,content=content)
            yield item
        next_url = response.xpath("//ul[@class='pagination']/li[last()]/a/@href").get()
        if not next_url:
            return
        else:
            yield scrapy.Request(self.base_domain + next_url, self.parse)
```

##### items.py éƒ¨åˆ†ä»£ç 

```python
import scrapy

class QsbkItem(scrapy.Item):
    author = scrapy.Field()
    content = scrapy.Field()
```

##### pipeline éƒ¨åˆ†ä»£ç 

```python
# æ–¹å¼1
import json
class QsbkPipeline(object):
    def __init__(self):
        self.fp = open("duanzi.josn", 'w', encoding='utf-8')

    def open_spider(self, spider):
        print('start...')

    def process_item(self, item, spider):
        item_json = json.dumps(dict(item), ensure_ascii=False)
        self.fp.write(item_json+ '\n')
        return item

    def close_spider(self, spider):
        self.fp.close()
        print('end...')

# æ–¹å¼2
from scrapy.exporters import JsonItemExporter
class QsbkPipeline(object):
    def __init__(self):
        self.fp = open("duanzi.josn", 'wb')
        self.exporter = JsonItemExporter(self.fp, ensure_ascii=False, encoding='utf-8')
        self.exporter.start_exporting()

    def open_spider(self, spider):
        print('start...')

    def process_item(self, item, spider):
        self.exporter.export_item(item)
        return item

    def close_spider(self, spider):
        self.exporter.finish_exporting()
        self.fp.close()
        print('end...')

# æ–¹å¼3
from scrapy.exporters import JsonLinesItemExporter
class QsbkPipeline(object):
    def __init__(self):
        self.fp = open("duanzi.josn", 'wb')
        self.exporter = JsonLinesItemExporter(self.fp, ensure_ascii=False, encoding='utf-8')

    def open_spider(self, spider):
        print('start...')

    def process_item(self, item, spider):
        self.exporter.export_item(item)
        return item

    def close_spider(self, spider):
        self.fp.close()
        print('end...')
```

#### è¿è¡Œ scrapy é¡¹ç›®

è¿è¡Œ scrapy é¡¹ç›®ã€‚éœ€è¦åœ¨ç»ˆç«¯ï¼Œè¿›å…¥é¡¹ç›®æ‰€åœ¨çš„è·¯å¾„ï¼Œç„¶å `scrapy crawl [çˆ¬è™«åå­—]` å³å¯è¿è¡ŒæŒ‡å®šçš„çˆ¬è™«ã€‚å¦‚æœä¸æƒ³æ¯æ¬¡éƒ½åœ¨å‘½ä»¤è¡Œä¸­è¿è¡Œï¼Œé‚£ä¹ˆå¯ä»¥æŠŠè¿™ä¸ªå‘½ä»¤å†™åœ¨ä¸€ä¸ªæ–‡ä»¶ä¸­ã€‚ä»¥åå°±åœ¨ pycharm ä¸­æ‰§è¡Œè¿è¡Œè¿™ä¸ªæ–‡ä»¶å°±å¯ä»¥äº†ã€‚æ¯”å¦‚ç°åœ¨æ–°åˆ›å»ºä¸€ä¸ªæ–‡ä»¶å«åš start.pyï¼Œç„¶ååœ¨è¿™ä¸ªæ–‡ä»¶ä¸­å¡«å…¥ä»¥ä¸‹ä»£ç ï¼š

```python
from scrapy import cmdline

cmdline.execute("scrapy crawl qsbk".split())
```

## JsonItemExporter å’Œ JsonLinesItemExporter

- ä¿å­˜ json æ•°æ®çš„æ—¶å€™ï¼Œå¯ä»¥ä½¿ç”¨è¿™ä¸¤ä¸ªç±»ï¼Œè®©æ“ä½œå˜å¾—æ›´ç®€å•
- `JsonItemExporter`ï¼šæ¯æ¬¡æŠŠæ•°æ®æ·»åŠ åˆ°å†…å­˜ä¸­ï¼Œæœ€åç»Ÿä¸€å†™å…¥ç£ç›˜ï¼Œå­˜å‚¨çš„æ•°æ®æ˜¯ä¸€ä¸ªæ»¡è¶³ json è§„åˆ™çš„æ•°æ®ï¼Œæ•°æ®é‡æ¯”è¾ƒå¤§ï¼Œæ¯”è¾ƒè€—å†…å­˜
- `JsonLinesItemExporter`ï¼šæ¯æ¬¡è°ƒç”¨`export_item`çš„æ—¶å€™æŠŠè¿™ä¸ª item å­˜å‚¨åˆ°ç£ç›˜ï¼Œæ¯ä¸€ä¸ªå­—å…¸æ˜¯ä¸€è¡Œï¼Œæ•´ä¸ªæ–‡ä»¶ä¸æ˜¯ä¸€ä¸ªæ»¡è¶³ json æ ¼å¼çš„æ–‡ä»¶ï¼Œæ¯æ¬¡å¤„ç†åˆçº§çš„æ—¶å€™ç›´æ¥å­˜å‚¨åˆ°ç¡¬ç›˜ï¼Œä¸è€—å†…å­˜ï¼Œæ•°æ®æ¯”è¾ƒå®‰å…¨

## Scrapy çˆ¬è™«æ³¨æ„äº‹é¡¹

- response æ˜¯ä¸€ä¸ª`from scrapy.http.response.html.HtmlResponse`å¯¹è±¡ï¼Œå¯ä»¥æ‰§è¡Œ`xpath`å’Œ`css`è¯­æ³•æå–æ•°æ®
- æå–å‡ºæ¥çš„æ•°æ®æ˜¯ä¸€ä¸ª`Selector`æˆ–è€…`SelectorList`å¯¹è±¡ï¼Œå¦‚æœæƒ³è¦è·å–å…¶ä¸­çš„å­—ç¬¦ä¸²ï¼Œåº”è¯¥æ‰§è¡Œ`getall`æˆ–è€…`get`æ–¹æ³•
- getall æ–¹æ³•ï¼šè·å–`Selector`ä¸­æ‰€æœ‰æ–‡æœ¬ï¼Œè¿”å›çš„æ˜¯ä¸€ä¸ªåˆ—è¡¨
- get æ–¹æ³•ï¼šè·å–çš„æ˜¯`Selector`ä¸­çš„ç¬¬ä¸€ä¸ªæ–‡æœ¬ï¼Œè¿”å›çš„æ˜¯ str ç±»å‹
- å¦‚æœæ•°æ®è§£æå›æ¥è¦ä¼ ç»™ pipelines å¤„ç†ï¼Œå¯ä»¥ä½¿ç”¨`yield`æ¥è¿”å›ï¼Œæˆ–è€…æ˜¯æ·»åŠ æ‰€æœ‰çš„ itemï¼Œç»Ÿä¸€ä½¿ç”¨`return`è¿”å›
- itemï¼šåœ¨`item.py`ä¸­å®šä¹‰å¥½æ¨¡å‹ï¼Œä¸è¦ä½¿ç”¨å­—å…¸
- pipelinesï¼šè¿™ä¸ªæ˜¯ä¸“é—¨ä¸€ä»æ¥ä¿å­˜æ•°æ®çš„ï¼Œå…¶ä¸­æœ‰ä¸‰ä¸ªæ–¹æ³•æ˜¯ä¼šè¢«ç»å¸¸ç”¨åˆ°çš„ã€‚è¦æ¿€æ´» pipelinesï¼Œåº”è¯¥åœ¨`settings.py`ä¸­ï¼Œè®¾ç½®`ITEM_PIPELINES`
  - `open_spider`ï¼šå½“çˆ¬è™«è¢«æ‰“å¼€çš„æ—¶å€™æ‰§è¡Œ
  - `process_item`ï¼šå½“çˆ¬è™«æœ‰ item ä¼ è¿‡æ¥çš„æ—¶å€™ä¼šè¢«è°ƒç”¨
  - `close_spider`ï¼šå½“çˆ¬è™«å…³é—­çš„æ—¶å€™è¢«è°ƒç”¨

## CrawlSpider

åœ¨ç³—äº‹ç™¾ç§‘çš„çˆ¬è™«æ¡ˆä¾‹ä¸­ã€‚æˆ‘ä»¬æ˜¯è‡ªå·±åœ¨è§£æå®Œæ•´ä¸ªé¡µé¢åè·å–ä¸‹ä¸€é¡µçš„ urlï¼Œç„¶åé‡æ–°å‘é€ä¸€ä¸ªè¯·æ±‚ã€‚æœ‰æ—¶å€™æˆ‘ä»¬æƒ³è¦è¿™æ ·åšï¼Œåªè¦æ»¡è¶³æŸä¸ªæ¡ä»¶çš„ urlï¼Œéƒ½ç»™æˆ‘è¿›è¡Œçˆ¬å–ã€‚é‚£ä¹ˆè¿™æ—¶å€™æˆ‘ä»¬å°±å¯ä»¥é€šè¿‡ CrawlSpider æ¥å¸®æˆ‘ä»¬å®Œæˆäº†ã€‚CrawlSpider ç»§æ‰¿è‡ª Spiderï¼Œåªä¸è¿‡æ˜¯åœ¨ä¹‹å‰çš„åŸºç¡€ä¹‹ä¸Šå¢åŠ äº†æ–°çš„åŠŸèƒ½ï¼Œå¯ä»¥å®šä¹‰çˆ¬å–çš„ url çš„è§„åˆ™ï¼Œä»¥å scrapy ç¢°åˆ°æ»¡è¶³æ¡ä»¶çš„ url éƒ½è¿›è¡Œçˆ¬å–ï¼Œè€Œä¸ç”¨æ‰‹åŠ¨çš„ yield Requestã€‚

## åˆ›å»º CrawlSpider çˆ¬è™«

ä¹‹å‰åˆ›å»ºçˆ¬è™«çš„æ–¹å¼æ˜¯é€šè¿‡`scrapy genspider [çˆ¬è™«åå­—] [åŸŸå]`çš„æ–¹å¼åˆ›å»ºçš„ã€‚å¦‚æœæƒ³è¦åˆ›å»º CrawlSpider çˆ¬è™«ï¼Œé‚£ä¹ˆåº”è¯¥é€šè¿‡ä»¥ä¸‹å‘½ä»¤åˆ›å»ºï¼š

`scrapy genspider -c crawl [çˆ¬è™«åå­—] [åŸŸå]`

## LinkExtractors é“¾æ¥æå–å™¨

ä½¿ç”¨ LinkExtractors å¯ä»¥ä¸ç”¨ç¨‹åºå‘˜è‡ªå·±æå–æƒ³è¦çš„ urlï¼Œç„¶åå‘é€è¯·æ±‚ã€‚è¿™äº›å·¥ä½œéƒ½å¯ä»¥äº¤ç»™ LinkExtractorsï¼Œä»–ä¼šåœ¨æ‰€æœ‰çˆ¬çš„é¡µé¢ä¸­æ‰¾åˆ°æ»¡è¶³è§„åˆ™çš„ urlï¼Œå®ç°è‡ªåŠ¨çš„çˆ¬å–ã€‚

```python
class scrapy.linkextractors.LinkExtractor(
    allow = (),
    deny = (),
    allow_domains = (),
    deny_domains = (),
    deny_extensions = None,
    restrict_xpaths = (),
    tags = ('a','area'),
    attrs = ('href'),
    canonicalize = True,
    unique = True,
    process_value = None
)
```

- allowï¼šå…è®¸çš„ urlã€‚æ‰€æœ‰æ»¡è¶³è¿™ä¸ªæ­£åˆ™è¡¨è¾¾å¼çš„ url éƒ½ä¼šè¢«æå–ã€‚
- denyï¼šç¦æ­¢çš„ urlã€‚æ‰€æœ‰æ»¡è¶³è¿™ä¸ªæ­£åˆ™è¡¨è¾¾å¼çš„ url éƒ½ä¸ä¼šè¢«æå–ã€‚
- allow_domainsï¼šå…è®¸çš„åŸŸåã€‚åªæœ‰åœ¨è¿™ä¸ªé‡Œé¢æŒ‡å®šçš„åŸŸåçš„ url æ‰ä¼šè¢«æå–ã€‚
- deny_domainsï¼šç¦æ­¢çš„åŸŸåã€‚æ‰€æœ‰åœ¨è¿™ä¸ªé‡Œé¢æŒ‡å®šçš„åŸŸåçš„ url éƒ½ä¸ä¼šè¢«æå–ã€‚
- restrict_xpathsï¼šä¸¥æ ¼çš„ xpathã€‚å’Œ allow å…±åŒè¿‡æ»¤é“¾æ¥ã€‚

## Rule è§„åˆ™ç±»

å®šä¹‰çˆ¬è™«çš„è§„åˆ™ç±»ã€‚

```python
class scrapy.spiders.Rule(
    link_extractor,
    callback = None,
    cb_kwargs = None,
    follow = None,
    process_links = None,
    process_request = None
)
```

- link_extractorï¼šä¸€ä¸ª LinkExtractor å¯¹è±¡ï¼Œç”¨äºå®šä¹‰çˆ¬å–è§„åˆ™ã€‚
- callbackï¼šæ»¡è¶³è¿™ä¸ªè§„åˆ™çš„ urlï¼Œåº”è¯¥è¦æ‰§è¡Œå“ªä¸ªå›è°ƒå‡½æ•°ã€‚å› ä¸º CrawlSpider ä½¿ç”¨äº† parse ä½œä¸ºå›è°ƒå‡½æ•°ï¼Œå› æ­¤ä¸è¦è¦†ç›– parse ä½œä¸ºå›è°ƒå‡½æ•°è‡ªå·±çš„å›è°ƒå‡½æ•°ã€‚
- followï¼šæŒ‡å®šæ ¹æ®è¯¥è§„åˆ™ä» response ä¸­æå–çš„é“¾æ¥æ˜¯å¦éœ€è¦è·Ÿè¿›ã€‚
- process_linksï¼šä» link_extractor ä¸­è·å–åˆ°é“¾æ¥åä¼šä¼ é€’ç»™è¿™ä¸ªå‡½æ•°ï¼Œç”¨æ¥è¿‡æ»¤ä¸éœ€è¦çˆ¬å–çš„é“¾æ¥ã€‚

## Scrapy Shell

æˆ‘ä»¬æƒ³è¦åœ¨çˆ¬è™«ä¸­ä½¿ç”¨ xpathã€beautifulsoupã€æ­£åˆ™è¡¨è¾¾å¼ã€css é€‰æ‹©å™¨ç­‰æ¥æå–æƒ³è¦çš„æ•°æ®ã€‚ä½†æ˜¯å› ä¸º scrapy æ˜¯ä¸€ä¸ªæ¯”è¾ƒé‡çš„æ¡†æ¶ã€‚æ¯æ¬¡è¿è¡Œèµ·æ¥éƒ½è¦ç­‰å¾…ä¸€æ®µæ—¶é—´ã€‚å› æ­¤è¦å»éªŒè¯æˆ‘ä»¬å†™çš„æå–è§„åˆ™æ˜¯å¦æ­£ç¡®ï¼Œæ˜¯ä¸€ä¸ªæ¯”è¾ƒéº»çƒ¦çš„äº‹æƒ…ã€‚å› æ­¤ Scrapy æä¾›äº†ä¸€ä¸ª shellï¼Œç”¨æ¥æ–¹ä¾¿çš„æµ‹è¯•è§„åˆ™

æ‰“å¼€ cmd ç»ˆç«¯ï¼Œè¿›å…¥åˆ° Scrapy é¡¹ç›®æ‰€åœ¨çš„ç›®å½•ï¼Œç„¶åè¿›å…¥åˆ° scrapy æ¡†æ¶æ‰€åœ¨çš„è™šæ‹Ÿç¯å¢ƒä¸­ï¼Œè¾“å…¥å‘½ä»¤`scrapy shell [é“¾æ¥]`ã€‚å°±ä¼šè¿›å…¥åˆ° scrapy çš„ shell ç¯å¢ƒä¸­ã€‚åœ¨è¿™ä¸ªç¯å¢ƒä¸­ï¼Œä½ å¯ä»¥è·Ÿåœ¨çˆ¬è™«çš„ parse æ–¹æ³•ä¸­ä¸€æ ·ä½¿ç”¨äº†ã€‚
